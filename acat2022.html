<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <link rel="stylesheet" href="./scripts/css/styles.css">
        <link rel="icon" href="./img/SYCL_logo.png">
        <title> SYCL @ CMS </title>
    </head>
    <body>
        <!--===== HEADER =====-->
        <header class="l-header">
            <img src="img/cms_experiment_small.jpg" alt="Simply Easy Learning" width="2.5rem" height="2.5rem">
            <nav class="nav bd-grid">
                <div class="nav__img">
                    <img src="./img/logo.png" alt="">
                </div>
                <div>
                    <a href="#" class="nav__logo">SYCL for event reconstruction at CMS</a>
                </div>

                <div class="nav__menu" id="nav-menu">
                    <ul class="nav__list">
                        <li class="nav__item"><a href="#heterogeneous" class="nav__link">Heterogeneous computing</a></li>
                        <li class="nav__item"><a href="#portability" class="nav__link">Performance portability</a></li>
                        <li class="nav__item"><a href="#porting" class="nav__link">CUDA2SYCL</a></li>
                        <li class="nav__item"><a href="#performance" class="nav__link">Performance</a></li>
                        <li class="nav__item"><a href="#link" class="nav__link">Reference</a></li>
                    </ul>
                </div>

                <div class="nav__toggle" id="nav-toggle">
                    <i class='bx bx-menu'></i>
                </div>
            </nav>
        </header>

        <main class="l-main">
            <!--===== HOME =====-->
            <section class="home section " id="home">
		   <h1 class="home__title">Experience in SYCL/oneAPI<br>for event reconstruction<br>at the CMS experiment</h1>
	    </section>

            <!--===== Heterogeneous =====-->
            <section class="about section " id="heterogeneous">
                <h2 class="section-title">Heterogenous computing at the CMS experiment</h2>
                <hr style="height:0.25rem;background-color:#4070F4;color:#4070F4;width:100px">  
                <div class="about__container bd-grid">
                    <div>
                        <p class="about__text">
                            In the High-Luminosity phase of the LHC (HL-LHC) the accelerator will reach an instantaneous luminosity of 7 Ã— 10<sup>34</sup> cm<sup>-2</sup>s<sup>-1</sup> with an average pileup of 200 proton-proton collisions. This will lead to a <strong>computational challenge</strong> for the online and offline reconstruction software that have been developed. To face this complexity CMS has decided to use <strong>heterogeneous computing</strong>, meaning that different types of architectures will be used instead of CPUs only.<br>
                            To accomplish this task, before the start of Run-3 the CMS experiment has equipped the HLT nodes with NVIDIA Tesla T4 GPUs that are currently used in production for:
                            <ol>
                              <li>pixel unpacking, local reconstruction, tracks and vertices</li>
                              <li>ECAL unpacking and local reconstruction</li>
                              <li>HCAL local reconstruction</li>
                            </ol>
                            This choice has resulted in:
                            <ol>
                              <li><strong>higher throughput</strong> thanks to the use of accelerators;</li>
                              <li><strong>better physics performance</strong> due to the fundamental redesign of the algorithms to fully exploit the parallelism capabilities of GPUs.</li>
                            </ol>
                            The goal for the HL-LHC is to offload at least 50% of the HLT computations to GPUs in Run-4 scaling up to 80% in Run-5.
                        </p>           
                    </div>  
                </div>                                   
            </section>
            <!--===== Portability =====-->
            <section class="about section " id="portability">
		    <h2 class="section-title">Performance portability libraries</h2>
            <hr style="height:0.25rem;background-color:#4070F4;color:#4070F4;width:100px"> 
                <div class="about__container bd-grid">
                    <div class="about__img">
                        <img src="./img/dpcpp.png" alt="">
                    </div>   
                    <div>
                        <p class="about__text">
                            Currently, the code to be executed on GPUs is written in CUDA specifically for NVIDIA GPUs. Through a custom-built compatibility layer, the same code can run entirely on CPUs, if no GPU is available. In this way, to offload the computational work on different non-CPU resources, different implementations of the same code are required. This approach should be avoided because it would introduce <strong>code-duplication</strong> that is <strong>not easily maintainable</strong>. A possible solution is the use of performance portability libraries, that allow the programmer to write a <strong>single source code</strong> which can be executed on different architectures.<br>
                            The CMS experiment has evaluated some performance portability libraries and Alpaka has been chosen for Run-3. The migration from CUDA to Alpaka at the HLT will be performed in the next few months. Despite this, studies on this kind of libraries are still ongoing and other solutions are being explored.<br>
                            One possibility is to use <strong>Data Parallel C++ (DPC++)</strong>, an open-source compiler project developed by Intel, that is part of the oneAPI programming model. DPC++ is based on SYCL, a cross-platform abstraction layer maintained by Khronos Group that allows code to be written using standard ISO C++ both for the host and the device in the same source file.<br>
                            At the moment DPC++ officially supports CPUs, Intel GPUs and Intel FPGAs. The support for NVIDIA and AMD GPUs is in active development, anyway it's possible to use them with the Intel's LLVM Compiler. 
                        </p>           
                    </div>                           
                </div>
            </section>

            <!--===== Porting =====-->
            <section class="about section " id="porting">
                <h2 class="section-title">From CUDA to SYCL</h2>
                <hr style="height:0.25rem;background-color:#4070F4;color:#4070F4;width:100px"> 
                <div class="about__container bd-grid">
                    <div>
                        <p class="about__text">
                            Starting from code written in CUDA, the first step in the study of SYCL/oneAPI consisted in writing a SYCL version of two algorithms:
                            <ol>
                                <li><a href="#clue"><strong>CLUE (CLUstering of Energy)</strong></a>: a fast parallel clustering algorithm for high granularity calorimeters that follows a density-based approach to link each hit with its nearest neighbour with higher energy density;</li>
                                <li><a href="#pixeltrack"><strong>pixeltrack</strong></a>: a heterogeneous implementation of pixel tracks and vertices reconstruction chain, starting from the detector raw data, creating clusters, then n-tuplets that are fitted to obtain tracks, used to reconstruct the vertices.</li>
                            </ol>
                            To ease the process of porting the code from CUDA to SYCL, Intel made available the Data Parallel Compatibility Tool (DPCT). It was used partially at the beginning to gain confidence with the SYCL language and turned out to be helpful even though its output code still needed revision.
                            During the porting we had the possibility to explore SYCL and to compare it with CUDA. The main logic is the same, but there are some <strong>differences</strong>:
                            <ol>
                                <li>each device is associated to a queue (analogue of a CUDA stream) that is needed to launch kernels and to do all the memory operations;</li>
                                <li>the use of different terms (e.g. thread, block, grid become work-item, work-group, Nd range) or of the same term but with a different meaning;</li>
                                <li>the need to avoid hardware specific code, to enhance its portability;</li>
                                <li>some features like pinned memory are not explicitly available in SYCL as they are managed by the SYCL runtime.</li>
                            </ol>
                            These differences are mainly related to the fact that SYCL is a <strong>higher-level abstraction layer</strong> to ensure <strong>portability between devices</strong>. For example, regarding the first point, the queue is compulsory for SYCL to know on which device the operation has to be executed, because in principle different types of accelerators can be used inside the same code. This is not the case in CUDA, so additional attention must be posed on that during the porting.
                        </p>
                    </div> 
                    <div class="about__img">
                        <img src="./img/reco_steps.png" alt="">
                    </div>                                      
                </div>
            </section>

            <!--===== Performance =====-->
            <section class="about section " id="performance">
                <h2 class="section-title">Performance tests and conclusions</h2>
                <hr style="height:0.25rem;background-color:#4070F4;color:#4070F4;width:100px"> 
                <div class="work__container bd-grid">
                    <div class="work__img">
                        <img src="./img/performance_intel.png" alt="">
                    </div>                                 
                    <div class="work__img">
                        <img src="./img/performance_T4.png" alt="">
                    </div>                    <div>
                        <p class="about__text">
                            The porting of pixeltrack is still ongoing, but we are able to successfully run the code both on Intel CPUs and Intel GPUs. On the other hand CLUE has been fully ported and tested. 
                            To carry out the tests, we used a synthetic dataset that simulates the expected conditions in high granularity calorimeters operated at HL-LHC. It represents a calorimeter with 100 sensor layers. 10000 points have been generated on each layer such that the density represents circular clusters of energy whose magnitude decreases radially from the centre of the cluster according to a Gaussian distribution with the standard deviation, Ïƒ, set to 3 cm. 5% of the points represent noise distributed uniformly over the layers. 
                            The plots show the total throughput for different numbers of concurrent threads/streams.<br>
                            At first we tested it on Intel CPU and GPU, showing that there is an actual gain in performance using the accelerator with respect to the traditional processor. Then we also tested it on a TESLA T4 NVIDIA GPU and compared its performance with native CUDA and Alpaka.<br>
                            The results look really promising, showing that SYCL is able to reach a throughput similar to native CUDA.<br>
                            <br>
                            </p>           
                    </div>  
		</div>
                <p class="about__text bd-grid">
		To conclude, heterogeneous computing can be a solution to face the computational challenge and a compatibility layer will help in reproducibility of the results and code maintainability, reaching about the same throughput as the native implementation. SYCL is promising from this point of view and the first tests show really good performance, but it's not yet ready for a full-scale implementation since some features are still under development. 
		</p>
            </section>

            <!--===== References and links =====-->
            <section class="about section " id="link">
                <h2 class="section-title">References</h2>
                <hr style="height:0.25rem;background-color:#4070F4;color:#4070F4;width:100px"> 
                    <div>
                        <p>
                            <ol>
                                <li id="pixeltrack">pixeltrack-standalone - <a href="https://github.com/AuroraPerego/pixeltrack-standalone/tree/dev">GitHub repo</a></li>
                                <li>pixeltrack-standalone - <a href="https://doi.org/10.3389/fdata.2020.601728">paper</a></li>
                                <li id="clue">heterogeneous clue - <a href="https://github.com/cms-patatrack/heterogeneous-clue">GitHub repo</a></li>
                                <li>heterogeneous clue - <a href="https://doi.org/10.3389/fdata.2020.591315">paper</a></li>
                                <li><a href="https://patatrack.web.cern.ch/patatrack/wiki/cuda2sycl_rules/">porting CUDA to SYCL guide</a></li>
                                <li><a href="https://link.springer.com/book/10.1007/978-1-4842-5574-2">DataParallel C++</a></li>
                            </ol>
                        </p>
                    </div>
            </section>
            <!--===== FOOTER =====-->
        <footer class="footer">
            <p class="footer__title">Authors</p>
	    <p class="about__text"> Nikolaos Andriotis<sup>1</sup>, Andrea Bocci<sup>2</sup>, Laura Cappelli<sup>3</sup>, Tony Di Pilato<sup>4,9</sup>, Luca Ferragina<sup>5</sup>, Juan Jose Olivera Loyola<sup>6</sup>, Felice Pantaleo<sup>2</sup>, <strong>Aurora Perego</strong><sup>7</sup>, Wahid Redjeb<sup>2,8</sup><br><br>
            <sup>1</sup>Barcelona Supercomputing Center, Spain<br>
            <sup>2</sup>CERN, Switzerland<br>
            <sup>3</sup>INFN Bologna, Italy<br>
            <sup>4</sup>Center for Advanced Systems Understanding (CASUS)<br>
            <sup>5</sup>University of Bologna, Italy<br>
            <sup>6</sup>Institute of Technology and Higher Studies of Monterrey, Mexico<br>
            <sup>7</sup>University of Milano Bicocca, Italy<br>
            <sup>8</sup>RWTH Aachen University, Germany<br>
            <sup>9</sup>University of Geneva, Switzerland<br>
        </p>
            <div class="footer__social">
                <a href="https://github.com/cms-patatrack" class="footer__icon"><i class='bx bxl-github' ></i></a>
            </div>
        </footer>
	
	<!--===== SCROLL REVEAL =====-->
        <script src="https://unpkg.com/scrollreveal"></script>

        <!--===== MAIN JS =====-->
        <script src="./scripts/js/main.js"></script>
   
    </body>
</html>
